{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdd2d2a-5d9b-4848-acd9-35b198ce7df8",
   "metadata": {},
   "source": [
    "Notebook which sets up a final set of experiments, after performing hyperparameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8536d317-0afe-4470-b4ce-73bbb8758039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import BEBE.evaluation.evaluation as evaluation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e248c1-7aac-43ae-adf4-f58444ffc493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "\n",
    "##########\n",
    "final_experiment_name = \"crows_vame_smooth\"\n",
    "model_selection_dir = Path('/home/jupyter/behavior_benchmarks_experiments/crows_vame_smooth_model_selection') # Directory where hyperparameter search was performed\n",
    "final_experiment_dir_parent = \"/home/jupyter/behavior_benchmarks_experiments\"\n",
    "n_experiments = 3\n",
    "#########\n",
    "\n",
    "final_experiment_dir = os.path.join(final_experiment_dir_parent, final_experiment_name)\n",
    "\n",
    "if not os.path.exists(final_experiment_dir):\n",
    "    os.makedirs(final_experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cfcead3-e40b-4c33-852f-33f93580fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters based on best average macro f1 score (on dev or val set, for unsupervised or supervised models, respectively)\n",
    "\n",
    "# Check if supervised or unsupervised\n",
    "config_fp = sorted(model_selection_dir.glob('*/config.yaml'))[0]\n",
    "with open(config_fp, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "if config['unsupervised']:\n",
    "    results = model_selection_dir.glob('**/dev_eval.yaml')\n",
    "else:\n",
    "    results = model_selection_dir.glob('**/val_eval.yaml')\n",
    "    \n",
    "best_experiment = None\n",
    "best_f1 = -1\n",
    "for x in results:\n",
    "    with open(x, 'r') as f:\n",
    "        y = yaml.safe_load(f)\n",
    "    mean_f1 = np.mean(y['individual_scores']['macro_f1s'])\n",
    "    if mean_f1 > best_f1:\n",
    "        best_f1 = mean_f1\n",
    "        best_experiment = x.parent\n",
    "\n",
    "# Copy selected hyperparameters\n",
    "        \n",
    "selected_config_fp = str(best_experiment) + '.yaml'\n",
    "with open(selected_config_fp, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['use_val_in_train'] = True # Enforce we train on all availabe data (only applies to supervised models)\n",
    "config['output_parent_dir'] = final_experiment_dir\n",
    "\n",
    "# Save off config files for final experiments\n",
    "\n",
    "config_fps = []\n",
    "for i in range(n_experiments):\n",
    "    experiment_name = 'trial_' + str(i)\n",
    "    config['experiment_name'] = experiment_name\n",
    "    target_filename = experiment_name + '.yaml'\n",
    "    target_fp = os.path.join(final_experiment_dir, target_filename)                       \n",
    "    with open(target_fp, 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "    config_fps.append(target_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4ef46f8-3b90-4a44-ab19-006408186de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python full_experiment.py --config /home/jupyter/behavior_benchmarks_experiments/crows_vame_smooth/trial_0.yaml; python full_experiment.py --config /home/jupyter/behavior_benchmarks_experiments/crows_vame_smooth/trial_1.yaml; python full_experiment.py --config /home/jupyter/behavior_benchmarks_experiments/crows_vame_smooth/trial_2.yaml; \n"
     ]
    }
   ],
   "source": [
    "# Generate command line to run these experiments\n",
    "\n",
    "output = \"\"\n",
    "for config_fp in config_fps:\n",
    "    output += \"python full_experiment.py --config \" + config_fp + \"; \"\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330d716-38e8-48aa-9a0b-beb4681d5ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "bebe",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "BEBE",
   "language": "python",
   "name": "bebe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
