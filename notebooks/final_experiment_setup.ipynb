{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffdd2d2a-5d9b-4848-acd9-35b198ce7df8",
   "metadata": {},
   "source": [
    "Notebook which sets up a final set of experiments, after performing hyperparameter selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8536d317-0afe-4470-b4ce-73bbb8758039",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import BEBE.evaluation.evaluation as evaluation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e248c1-7aac-43ae-adf4-f58444ffc493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify directories\n",
    "\n",
    "##########\n",
    "final_experiment_name = \"gulls_wavelet_kmeans\"\n",
    "model_selection_dir = Path('/home/jupyter/behavior_benchmarks_experiments/gulls_wavelet_kmeans_model_selection') # Directory where hyperparameter search was performed\n",
    "final_experiment_dir_parent = \"/home/jupyter/behavior_benchmarks_experiments\"\n",
    "n_experiments = 3\n",
    "#########\n",
    "\n",
    "final_experiment_dir = os.path.join(final_experiment_dir_parent, final_experiment_name)\n",
    "\n",
    "if not os.path.exists(final_experiment_dir):\n",
    "    os.makedirs(final_experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "304ea9b3-f808-445f-962c-07b0380e10cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legacy:\n",
    "# # Re-run evaluation in the new style\n",
    "# for x in tqdm(model_selection_dir.glob('*/config.yaml')):\n",
    "#     with open(x, 'r') as f:\n",
    "#         config = yaml.safe_load(f)\n",
    "#     if config['unsupervised']:\n",
    "#         eval_fp = Path(x.parent, 'dev_eval.yaml')\n",
    "#     else:\n",
    "#         eval_fp = Path(x.parent, 'val_eval.yaml')\n",
    "#     if os.path.exists(eval_fp):\n",
    "#         with open(eval_fp, 'r') as f:\n",
    "#             evaluation_file = yaml.safe_load(f)\n",
    "#         if 'individual_scores' not in evaluation_file:\n",
    "#             evaluation.generate_evaluations(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cfcead3-e40b-4c33-852f-33f93580fdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose hyperparameters based on best average macro f1 score (on dev or val set, for unsupervised or supervised models, respectively)\n",
    "\n",
    "if config['unsupervised']:\n",
    "    results = model_selection_dir.glob('**/dev_eval.yaml')\n",
    "else:\n",
    "    results = model_selection_dir.glob('**/val_eval.yaml')\n",
    "best_experiment = None\n",
    "best_f1 = -1\n",
    "for x in results:\n",
    "    with open(x, 'r') as f:\n",
    "        y = yaml.safe_load(f)\n",
    "    mean_f1 = np.mean(y['individual_scores']['macro_f1s'])\n",
    "    if 'supervised_scores' in y:\n",
    "        f1 = y['supervised_scores']['classification_f1_macro']\n",
    "    else:\n",
    "        f1 = y['MAP_scores']['MAP_classification_f1_macro']\n",
    "    if mean_f1 > best_f1:\n",
    "        best_f1 = mean_f1\n",
    "        best_experiment = x.parent\n",
    "\n",
    "# Copy selected hyperparameters\n",
    "        \n",
    "selected_config_fp = str(best_experiment) + '.yaml'\n",
    "with open(selected_config_fp, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "config['use_val_in_train'] = True #Enforce we train on all availabe data (only applies to supervised models)\n",
    "config['output_parent_dir'] = final_experiment_dir\n",
    "\n",
    "# Save off config files for final experiments\n",
    "\n",
    "config_fps = []\n",
    "for i in range(n_experiments):\n",
    "    experiment_name = 'trial_' + str(i)\n",
    "    config['experiment_name'] = experiment_name\n",
    "    target_filename = experiment_name + '.yaml'\n",
    "    target_fp = os.path.join(final_experiment_dir, target_filename)                       \n",
    "    with open(target_fp, 'w') as file:\n",
    "        yaml.dump(config, file)\n",
    "    config_fps.append(target_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4ef46f8-3b90-4a44-ab19-006408186de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python full_experiment.py --config /home/jupyter/behavior_benchmarks_experiments/gulls_wavelet_kmeans/trial_0.yaml; python full_experiment.py --config /home/jupyter/behavior_benchmarks_experiments/gulls_wavelet_kmeans/trial_1.yaml; python full_experiment.py --config /home/jupyter/behavior_benchmarks_experiments/gulls_wavelet_kmeans/trial_2.yaml; \n"
     ]
    }
   ],
   "source": [
    "# Generate command line to run these experiments\n",
    "\n",
    "output = \"\"\n",
    "for config_fp in config_fps:\n",
    "    output += \"python full_experiment.py --config \" + config_fp + \"; \"\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fd60a-dc0c-4ac0-935e-df1c3e4d1d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
