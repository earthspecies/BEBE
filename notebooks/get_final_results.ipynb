{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c85c02-2162-474a-a943-f1a2bda517c6",
   "metadata": {},
   "source": [
    "Notebook which computes scores based on final sets of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a385f52-fe62-4d0b-83ab-e9eab123cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import BEBE.evaluation.evaluation as evaluation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260daec6-9886-4cc8-9763-6ab3dc5e0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "exp_dir = Path(\"/home/jupyter/behavior_benchmarks_experiments/whales_wavelet_kmeans_v1\")\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c77fe920-2d0a-4757-9e09-3282bc174210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Legacy\n",
    "# # Re-run evaluation in the new style\n",
    "# for x in tqdm(exp_dir.glob('*/config.yaml')):\n",
    "#     with open(x, 'r') as f:\n",
    "#         config = yaml.safe_load(f)\n",
    "#     if config['unsupervised']:\n",
    "#         eval_fp = Path(x.parent, 'dev_eval.yaml')\n",
    "#     else:\n",
    "#         eval_fp = Path(x.parent, 'val_eval.yaml')\n",
    "    \n",
    "        \n",
    "#     # load metadata\n",
    "#     metadata_fp = os.path.join(config['dataset_dir'], 'dataset_metadata.yaml')\n",
    "#     with open(metadata_fp) as file:\n",
    "#         config['metadata'] = yaml.load(file, Loader=yaml.FullLoader)\n",
    "#     if os.path.exists(eval_fp):\n",
    "#         with open(eval_fp, 'r') as f:\n",
    "#             evaluation_file = yaml.safe_load(f)\n",
    "#         if 'time_scale_ratios' not in evaluation_file['individual_scores']:\n",
    "#             evaluation.generate_evaluations(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ba14a23-b9a2-4cb8-bcbf-c5df90eba73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test f1  : 0.373 (0.034)\n",
      "Test Prec: 0.836 (0.033)\n",
      "Test Rec : 0.431 (0.022)\n",
      "Test TSR : 0.108 (0.051)\n"
     ]
    }
   ],
   "source": [
    "fps = exp_dir.glob('**/test_eval.yaml')\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "tsrs = []\n",
    "for fp in fps:\n",
    "    with open(fp, 'r') as f:\n",
    "        x = yaml.safe_load(f)\n",
    "    n_individuals = len(x['individual_scores']['macro_f1s'])\n",
    "    f1s.extend(x['individual_scores']['macro_f1s'])\n",
    "    precs.extend(x['individual_scores']['macro_precisions'])\n",
    "    recs.extend(x['individual_scores']['macro_recalls'])\n",
    "    tsrs.extend(x['individual_scores']['time_scale_ratios'])\n",
    "#print(n_individuals)\n",
    "print(\"Test f1  : %1.3f (%1.3f)\" % (np.mean(f1s), np.std(f1s)))\n",
    "print(\"Test Prec: %1.3f (%1.3f)\" % (np.mean(precs), np.std(precs)))\n",
    "print(\"Test Rec : %1.3f (%1.3f)\" % (np.mean(recs), np.std(recs)))\n",
    "print(\"Test TSR : %1.3f (%1.3f)\" % (np.mean(tsrs), np.std(tsrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3c9141-a440-40e9-9d2c-1b7b358e0815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train f1  : 0.324 (0.029)\n",
      "Train Prec: 0.793 (0.031)\n",
      "Train Rec : 0.379 (0.033)\n",
      "Train TSR : 0.066 (0.020)\n"
     ]
    }
   ],
   "source": [
    "fps = exp_dir.glob('**/dev_eval.yaml')\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "tsrs = []\n",
    "for fp in fps:\n",
    "    with open(fp, 'r') as f:\n",
    "        x = yaml.safe_load(f)\n",
    "    n_individuals = len(x['individual_scores']['macro_f1s'])\n",
    "    f1s.extend(x['individual_scores']['macro_f1s'])\n",
    "    precs.extend(x['individual_scores']['macro_precisions'])\n",
    "    recs.extend(x['individual_scores']['macro_recalls'])\n",
    "    tsrs.extend(x['individual_scores']['time_scale_ratios'])\n",
    "#print(n_individuals)\n",
    "print(\"Train f1  : %1.3f (%1.3f)\" % (np.mean(f1s), np.std(f1s)))\n",
    "print(\"Train Prec: %1.3f (%1.3f)\" % (np.mean(precs), np.std(precs)))\n",
    "print(\"Train Rec : %1.3f (%1.3f)\" % (np.mean(recs), np.std(recs)))\n",
    "print(\"Train TSR : %1.3f (%1.3f)\" % (np.mean(tsrs), np.std(tsrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2574ce-4580-41c4-b0df-fa60db342eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
