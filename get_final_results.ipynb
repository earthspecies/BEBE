{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a385f52-fe62-4d0b-83ab-e9eab123cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import BEBE.evaluation.evaluation as evaluation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260daec6-9886-4cc8-9763-6ab3dc5e0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(\"/home/jupyter/behavior_benchmarks_experiments/gulls_umapper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77fe920-2d0a-4757-9e09-3282bc174210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model outputs to /home/jupyter/behavior_benchmarks_experiments/gulls_umapper/umapper1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [02:03, 123.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model outputs to /home/jupyter/behavior_benchmarks_experiments/gulls_umapper/umapper0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [04:08, 124.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model outputs to /home/jupyter/behavior_benchmarks_experiments/gulls_umapper/umapper2 \n"
     ]
    }
   ],
   "source": [
    "# Re-run evaluation in the new style\n",
    "for x in tqdm(exp_dir.glob('*/config.yaml')):\n",
    "    with open(x, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    if config['unsupervised']:\n",
    "        eval_fp = Path(x.parent, 'dev_eval.yaml')\n",
    "    else:\n",
    "        eval_fp = Path(x.parent, 'val_eval.yaml')\n",
    "    \n",
    "        \n",
    "    # load metadata\n",
    "    metadata_fp = os.path.join(config['dataset_dir'], 'dataset_metadata.yaml')\n",
    "    with open(metadata_fp) as file:\n",
    "        config['metadata'] = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    if os.path.exists(eval_fp):\n",
    "        with open(eval_fp, 'r') as f:\n",
    "            evaluation_file = yaml.safe_load(f)\n",
    "        if 'time_scale_ratios' not in evaluation_file['individual_scores']:\n",
    "            evaluation.generate_evaluations(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba14a23-b9a2-4cb8-bcbf-c5df90eba73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = exp_dir.glob('**/test_eval.yaml')\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "tsrs = []\n",
    "for fp in fps:\n",
    "    with open(fp, 'r') as f:\n",
    "        x = yaml.safe_load(f)\n",
    "    n_individuals = len(x['individual_scores']['macro_f1s'])\n",
    "    f1s.extend(x['individual_scores']['macro_f1s'])\n",
    "    precs.extend(x['individual_scores']['macro_precisions'])\n",
    "    recs.extend(x['individual_scores']['macro_recalls'])\n",
    "    tsrs.extend(x['individual_scores']['time_scale_ratios'])\n",
    "print(n_individuals)\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(f1s), np.std(f1s)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(precs), np.std(precs)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(recs), np.std(recs)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(tsrs), np.std(tsrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3c9141-a440-40e9-9d2c-1b7b358e0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = exp_dir.glob('**/dev_eval.yaml')\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "tsrs = []\n",
    "for fp in fps:\n",
    "    with open(fp, 'r') as f:\n",
    "        x = yaml.safe_load(f)\n",
    "    n_individuals = len(x['individual_scores']['macro_f1s'])\n",
    "    f1s.extend(x['individual_scores']['macro_f1s'])\n",
    "    precs.extend(x['individual_scores']['macro_precisions'])\n",
    "    recs.extend(x['individual_scores']['macro_recalls'])\n",
    "    tsrs.extend(x['individual_scores']['time_scale_ratios'])\n",
    "print(n_individuals)\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(f1s), np.std(f1s)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(precs), np.std(precs)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(recs), np.std(recs)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(tsrs), np.std(tsrs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2574ce-4580-41c4-b0df-fa60db342eaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-9.m82",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m82"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
