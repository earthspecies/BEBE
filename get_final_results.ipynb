{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a385f52-fe62-4d0b-83ab-e9eab123cffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import BEBE.evaluation.evaluation as evaluation\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "260daec6-9886-4cc8-9763-6ab3dc5e0b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_dir = Path(\"/home/jupyter/behavior_benchmarks_experiments/seals_vame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c77fe920-2d0a-4757-9e09-3282bc174210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model outputs to /home/jupyter/behavior_benchmarks_experiments/seals_vame/vame2 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:17, 17.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model outputs to /home/jupyter/behavior_benchmarks_experiments/seals_vame/vame0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:33, 16.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model outputs to /home/jupyter/behavior_benchmarks_experiments/seals_vame/vame1 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:49, 16.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Re-run evaluation in the new style\n",
    "for x in tqdm(exp_dir.glob('*/config.yaml')):\n",
    "    with open(x, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    if config['unsupervised']:\n",
    "        eval_fp = Path(x.parent, 'dev_eval.yaml')\n",
    "    else:\n",
    "        eval_fp = Path(x.parent, 'val_eval.yaml')\n",
    "    if os.path.exists(eval_fp):\n",
    "        with open(eval_fp, 'r') as f:\n",
    "            evaluation_file = yaml.safe_load(f)\n",
    "        if 'individual_scores' not in evaluation_file:\n",
    "            evaluation.generate_evaluations(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ba14a23-b9a2-4cb8-bcbf-c5df90eba73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.183 (0.047)\n",
      "0.694 (0.142)\n",
      "0.262 (0.025)\n"
     ]
    }
   ],
   "source": [
    "fps = exp_dir.glob('**/test_eval.yaml')\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "for fp in fps:\n",
    "    with open(fp, 'r') as f:\n",
    "        x = yaml.safe_load(f)\n",
    "    n_individuals = len(x['individual_scores']['macro_f1s'])\n",
    "    f1s.extend(x['individual_scores']['macro_f1s'])\n",
    "    precs.extend(x['individual_scores']['macro_precisions'])\n",
    "    recs.extend(x['individual_scores']['macro_recalls'])\n",
    "print(n_individuals)\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(f1s), np.std(f1s)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(precs), np.std(precs)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(recs), np.std(recs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea3c9141-a440-40e9-9d2c-1b7b358e0815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0.223 (0.104)\n",
      "0.693 (0.154)\n",
      "0.303 (0.104)\n"
     ]
    }
   ],
   "source": [
    "fps = exp_dir.glob('**/dev_eval.yaml')\n",
    "f1s = []\n",
    "precs = []\n",
    "recs = []\n",
    "for fp in fps:\n",
    "    with open(fp, 'r') as f:\n",
    "        x = yaml.safe_load(f)\n",
    "    n_individuals = len(x['individual_scores']['macro_f1s'])\n",
    "    f1s.extend(x['individual_scores']['macro_f1s'])\n",
    "    precs.extend(x['individual_scores']['macro_precisions'])\n",
    "    recs.extend(x['individual_scores']['macro_recalls'])\n",
    "print(n_individuals)\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(f1s), np.std(f1s)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(precs), np.std(precs)))\n",
    "print(\"%1.3f (%1.3f)\" % (np.mean(recs), np.std(recs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922672bc-33c5-4be9-9c9e-8c50751e73a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
